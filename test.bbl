\begin{thebibliography}{10}

\bibitem{tensorflow2015-whitepaper}
Mart\'{\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
  Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
  Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
  Levenberg, Dan Man\'{e}, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
  Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,
  Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\'{e}gas, Oriol
  Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang
  Zheng.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock Software available from tensorflow.org.

\bibitem{ahn2012bayesian}
Sungjin Ahn, Anoop Korattikara, and Max Welling.
\newblock Bayesian posterior sampling via stochastic gradient fisher scoring.
\newblock {\em arXiv preprint arXiv:1206.6380}, 2012.

\bibitem{bahrampour2015comparative}
Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott, and Mohak Shah.
\newblock Comparative study of deep learning software frameworks.
\newblock {\em arXiv preprint arXiv:1511.06435}, 2015.

\bibitem{bergstra2010theano}
James Bergstra, Olivier Breuleux, Fr{\'e}d{\'e}ric Bastien, Pascal Lamblin,
  Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and
  Yoshua Bengio.
\newblock Theano: A cpu and gpu math compiler in python.

\bibitem{besag1986statistical}
Julian Besag.
\newblock On the statistical analysis of dirty pictures.
\newblock {\em Journal of the Royal Statistical Society. Series B
  (Methodological)}, pages 259--302, 1986.

\bibitem{beskos2013optimal}
Alexandros Beskos, Natesh Pillai, Gareth Roberts, Jesus-Maria Sanz-Serna,
  Andrew Stuart, et~al.
\newblock Optimal tuning of the hybrid monte carlo algorithm.
\newblock {\em Bernoulli}, 19(5A):1501--1534, 2013.

\bibitem{betancourt2013general}
Michael Betancourt.
\newblock A general metric for riemannian manifold hamiltonian monte carlo.
\newblock In {\em Geometric science of information}, pages 327--334. Springer,
  2013.

\bibitem{betancourt2016identifying}
Michael Betancourt.
\newblock Identifying the optimal integration time in hamiltonian monte carlo.
\newblock {\em arXiv preprint arXiv:1601.00225}, 2016.

\bibitem{betancourt2013generalizing}
MJ~Betancourt.
\newblock Generalizing the no-u-turn sampler to riemannian manifolds.
\newblock {\em arXiv preprint arXiv:1304.1920}, 2013.

\bibitem{betancourt2014geometric}
MJ~Betancourt, Simon Byrne, Samuel Livingstone, and Mark Girolami.
\newblock The geometric foundations of hamiltonian monte carlo.
\newblock {\em arXiv preprint arXiv:1410.5110}, 2014.

\bibitem{bishop1995neural}
Christopher~M Bishop.
\newblock {\em Neural networks for pattern recognition}.
\newblock Oxford university press, 1995.

\bibitem{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock {\em arXiv preprint arXiv:1505.05424}, 2015.

\bibitem{boltz2007high}
Sylvain Boltz, Eric Debreuve, and Michel Barlaud.
\newblock High-dimensional statistical distance for region-of-interest
  tracking: Application to combining a soft geometric constraint with
  radiometry.
\newblock In {\em 2007 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 1--8. IEEE, 2007.

\bibitem{boltz2007knn}
Sylvain Boltz, Eric Debreuve, and Michel Barlaud.
\newblock knn-based high-dimensional kullback-leibler distance for tracking.
\newblock In {\em Image Analysis for Multimedia Interactive Services, 2007.
  WIAMIS'07. Eighth International Workshop on}, pages 16--16. IEEE, 2007.

\bibitem{burda2011bayesian}
Martin Burda, John Maheu, et~al.
\newblock Bayesian adaptive hamiltonian monte carlo with an application to
  high-dimensional bekk garch models.
\newblock Technical report, 2011.

\bibitem{carpenter2016stan}
Bob Carpenter.
\newblock Stan: A probabilistic programming language.

\bibitem{chen2015convergence}
Changyou Chen, Nan Ding, and Lawrence Carin.
\newblock On the convergence of stochastic gradient mcmc algorithms with
  high-order integrators.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2278--2286, 2015.

\bibitem{chen2014stochastic}
Tianqi Chen, Emily~B Fox, and Carlos Guestrin.
\newblock Stochastic gradient hamiltonian monte carlo.
\newblock In {\em ICML}, pages 1683--1691, 2014.

\bibitem{damianou2013deep}
Andreas~C Damianou and Neil~D Lawrence.
\newblock Deep gaussian processes.

\bibitem{dehaene2015expectation}
Guillaume Dehaene and Simon Barthelm{\'e}.
\newblock Expectation propagation in the large-data limit.
\newblock {\em arXiv preprint arXiv:1503.08060}, 2015.

\bibitem{ding2014bayesian}
Nan Ding, Youhan Fang, Ryan Babbush, Changyou Chen, Robert~D Skeel, and Hartmut
  Neven.
\newblock Bayesian sampling using stochastic gradient thermostats.
\newblock In {\em Advances in neural information processing systems}, pages
  3203--3211, 2014.

\bibitem{duane1987hybrid}
Simon Duane, Anthony~D Kennedy, Brian~J Pendleton, and Duncan Roweth.
\newblock Hybrid monte carlo.
\newblock {\em Physics letters B}, 195(2):216--222, 1987.

\bibitem{filippone2015enabling}
Maurizio Filippone and Raphael Engler.
\newblock Enabling scalable stochastic gradient-based inference for gaussian
  processes by employing the unbiased linear system solver (ulisse).

\bibitem{friedman2001elements}
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
\newblock {\em The elements of statistical learning}, volume~1.
\newblock Springer series in statistics Springer, Berlin, 2001.

\bibitem{gal2015dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.

\bibitem{gal2015bayesian}
Yarin Gal and Zoubin Ghahramani.
\newblock Bayesian convolutional neural networks with bernoulli approximate
  variational inference.
\newblock {\em arXiv preprint arXiv:1506.02158}, 2015.

\bibitem{gan2015scalable}
Zhe Gan, Changyou Chen, Ricardo Henao, and David Carlson.
\newblock Scalable deep poisson factor analysis for topic modeling.

\bibitem{gelman2014bayesian}
Andrew Gelman, John~B Carlin, Hal~S Stern, and Donald~B Rubin.
\newblock {\em Bayesian data analysis}, volume~2.
\newblock Chapman \& Hall/CRC Boca Raton, FL, USA, 2014.

\bibitem{gelman1996efficient}
Andrew Gelman, G~Roberts, and W~Gilks.
\newblock Efficient metropolis jumping hules.
\newblock 1996.

\bibitem{gelman2014expectation}
Andrew Gelman, Aki Vehtari, Pasi Jyl{\"a}nki, Christian Robert, Nicolas Chopin,
  and John~P Cunningham.
\newblock Expectation propagation as a way of life.
\newblock {\em arXiv preprint arXiv:1412.4869}, 2014.

\bibitem{geman1984stochastic}
Stuart Geman and Donald Geman.
\newblock Stochastic relaxation, gibbs distributions, and the bayesian
  restoration of images.
\newblock {\em IEEE Transactions on pattern analysis and machine intelligence},
  (6):721--741, 1984.

\bibitem{geyer1992practical}
Charles~J Geyer.
\newblock Practical markov chain monte carlo.
\newblock {\em Statistical Science}, pages 473--483, 1992.

\bibitem{ghosh2016assumed}
Soumya Ghosh, Francesco~Maria Delle~Fave, and Jonathan~S Yedidia.
\newblock Assumed density filtering methods for learning bayesian neural
  networks.
\newblock In {\em AAAI}, pages 1589--1595, 2016.

\bibitem{girolami2011riemann}
Mark Girolami and Ben Calderhead.
\newblock Riemann manifold langevin and hamiltonian monte carlo methods.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 73(2):123--214, 2011.

\bibitem{graves2011practical}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2348--2356, 2011.

\bibitem{green2015bayesian}
Peter~J Green, Krzysztof {\L}atuszy{\'n}ski, Marcelo Pereyra, and Christian~P
  Robert.
\newblock Bayesian computation: a summary of the current state, and samples
  backwards and forwards.
\newblock {\em Statistics and Computing}, 25(4):835--862, 2015.

\bibitem{guyon2004result}
Isabelle Guyon, Steve~R Gunn, Asa Ben-Hur, and Gideon Dror.
\newblock Result analysis of the nips 2003 feature selection challenge.
\newblock In {\em NIPS}, volume~4, pages 545--552, 2004.

\bibitem{hernandez2015probabilistic}
Jos{\'e}~Miguel Hern{\'a}ndez-Lobato and Ryan~P Adams.
\newblock Probabilistic backpropagation for scalable learning of bayesian
  neural networks.
\newblock {\em arXiv preprint arXiv:1502.05336}, 2015.

\bibitem{hinton1993keeping}
Geoffrey~E Hinton and Drew Van~Camp.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In {\em Proceedings of the sixth annual conference on Computational
  learning theory}, pages 5--13. ACM, 1993.

\bibitem{hoffman2014no}
Matthew~D Hoffman and Andrew Gelman.
\newblock The no-u-turn sampler: adaptively setting path lengths in hamiltonian
  monte carlo.
\newblock {\em Journal of Machine Learning Research}, 15(1):1593--1623, 2014.

\bibitem{hornik1991approximation}
Kurt Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock {\em Neural networks}, 4(2):251--257, 1991.

\bibitem{jia2014caffe}
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross
  Girshick, Sergio Guadarrama, and Trevor Darrell.
\newblock Caffe: Convolutional architecture for fast feature embedding.
\newblock In {\em Proceedings of the 22nd ACM international conference on
  Multimedia}, pages 675--678. ACM, 2014.

\bibitem{jylanki2014expectation}
Pasi Jyl{\"a}nki, Aapo Nummenmaa, and Aki Vehtari.
\newblock Expectation propagation for neural networks with sparsity-promoting
  priors.
\newblock {\em Journal of Machine Learning Research}, 15(1):1849--1901, 2014.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{kohavi1995study}
Ron Kohavi et~al.
\newblock A study of cross-validation and bootstrap for accuracy estimation and
  model selection.
\newblock In {\em Ijcai}, volume~14, pages 1137--1145. Stanford, CA, 1995.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{lampinen2001bayesian}
Jouko Lampinen and Aki Vehtari.
\newblock Bayesian approach for neural networksâ€”review and case studies.
\newblock {\em Neural networks}, 14(3):257--274, 2001.

\bibitem{le2012asymptotic}
Lucien Le~Cam.
\newblock {\em Asymptotic methods in statistical decision theory}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em Nature}, 521(7553):436--444, 2015.

\bibitem{leimkuhler2004simulating}
Benedict Leimkuhler and Sebastian Reich.
\newblock {\em Simulating hamiltonian dynamics}, volume~14.
\newblock Cambridge University Press, 2004.

\bibitem{li2015preconditioned}
Chunyuan Li, Changyou Chen, David Carlson, and Lawrence Carin.
\newblock Preconditioned stochastic gradient langevin dynamics for deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1512.07666}, 2015.

\bibitem{li2015high}
Chunyuan Li, Changyou Chen, Kai Fan, and Lawrence Carin.
\newblock High-order stochastic gradient thermostats for bayesian learning of
  deep models.
\newblock {\em arXiv preprint arXiv:1512.07662}, 2015.

\bibitem{li2015stochastic}
Yingzhen Li, Jos{\'e}~Miguel Hern{\'a}ndez-Lobato, and Richard~E Turner.
\newblock Stochastic expectation propagation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2323--2331, 2015.

\bibitem{liang2011advanced}
Faming Liang, Chuanhai Liu, and Raymond Carroll.
\newblock {\em Advanced Markov chain Monte Carlo methods: learning from past
  samples}, volume 714.
\newblock John Wiley \& Sons, 2011.

\bibitem{liu2008monte}
Jun~S Liu.
\newblock {\em Monte Carlo strategies in scientific computing}.
\newblock Springer Science \& Business Media, 2008.

\bibitem{livingstone2016geometric}
Samuel Livingstone, Michael Betancourt, Simon Byrne, and Mark Girolami.
\newblock On the geometric ergodicity of hamiltonian monte carlo.
\newblock {\em arXiv preprint arXiv:1601.08057}, 2016.

\bibitem{ma2015complete}
Yi-An Ma, Tianqi Chen, and Emily Fox.
\newblock A complete recipe for stochastic gradient mcmc.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2917--2925, 2015.

\bibitem{mackay1992evidence}
David~JC MacKay.
\newblock The evidence framework applied to classification networks.
\newblock {\em Neural computation}, 4(5):720--736, 1992.

\bibitem{mahendran2012adaptive}
Nimalan Mahendran, Ziyu Wang, Firas Hamze, and Nando De~Freitas.
\newblock Adaptive mcmc with bayesian optimization.

\bibitem{mandt2017stochastic}
Stephan Mandt, Matthew~D Hoffman, and David~M Blei.
\newblock Stochastic gradient descent as approximate bayesian inference.
\newblock {\em The Journal of Machine Learning Research}, 18(1):4873--4907,
  2017.

\bibitem{minka2001expectation}
Thomas~P Minka.
\newblock Expectation propagation for approximate bayesian inference.
\newblock In {\em Proceedings of the Seventeenth conference on Uncertainty in
  artificial intelligence}, pages 362--369. Morgan Kaufmann Publishers Inc.,
  2001.

\bibitem{montufar2014number}
Guido~F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio.
\newblock On the number of linear regions of deep neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  2924--2932, 2014.

\bibitem{nair2010rectified}
Vinod Nair and Geoffrey~E Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In {\em Proceedings of the 27th international conference on machine
  learning (ICML-10)}, pages 807--814, 2010.

\bibitem{neal1993bayesian}
Radford~M Neal.
\newblock Bayesian learning via stochastic dynamics.

\bibitem{neal1992improved}
Radford~M Neal.
\newblock An improved acceptance procedure for the hybrid monte carlo
  algorithm.
\newblock {\em arXiv preprint hep-lat/9208011}, 1992.

\bibitem{neal2012bayesian}
Radford~M Neal.
\newblock {\em Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem{neal2011mcmc}
Radford~M Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock {\em Handbook of Markov Chain Monte Carlo}, 2:113--162, 2011.

\bibitem{neiswanger2013asymptotically}
Willie Neiswanger, Chong Wang, and Eric Xing.
\newblock Asymptotically exact, embarrassingly parallel mcmc.
\newblock {\em arXiv preprint arXiv:1311.4780}, 2013.

\bibitem{ngiam2011optimization}
Jiquan Ngiam, Adam Coates, Ahbik Lahiri, Bobby Prochnow, Quoc~V Le, and
  Andrew~Y Ng.
\newblock On optimization methods for deep learning.
\newblock In {\em Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pages 265--272, 2011.

\bibitem{opper2009variational}
Manfred Opper and C{\'e}dric Archambeau.
\newblock The variational gaussian approximation revisited.
\newblock {\em Neural computation}, 21(3):786--792, 2009.

\bibitem{piironen2017sparsity}
Juho Piironen, Aki Vehtari, et~al.
\newblock Sparsity information and regularization in the horseshoe and other
  shrinkage priors.
\newblock {\em Electronic Journal of Statistics}, 11(2):5018--5051, 2017.

\bibitem{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock {\em arXiv preprint arXiv:1401.4082}, 2014.

\bibitem{ripley2007pattern}
Brian~D Ripley.
\newblock {\em Pattern recognition and neural networks}.
\newblock Cambridge university press, 2007.

\bibitem{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem{robert2013monte}
Christian Robert and George Casella.
\newblock {\em Monte Carlo statistical methods}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{roberts1997weak}
Gareth~O Roberts, Andrew Gelman, Walter~R Gilks, et~al.
\newblock Weak convergence and optimal scaling of random walk metropolis
  algorithms.
\newblock {\em The annals of applied probability}, 7(1):110--120, 1997.

\bibitem{roberts2009examples}
Gareth~O Roberts and Jeffrey~S Rosenthal.
\newblock Examples of adaptive mcmc.
\newblock {\em Journal of Computational and Graphical Statistics},
  18(2):349--367, 2009.

\bibitem{roberts2001optimal}
Gareth~O Roberts, Jeffrey~S Rosenthal, et~al.
\newblock Optimal scaling for various metropolis-hastings algorithms.
\newblock {\em Statistical science}, 16(4):351--367, 2001.

\bibitem{roberts2004general}
Gareth~O Roberts, Jeffrey~S Rosenthal, et~al.
\newblock General state space markov chains and mcmc algorithms.
\newblock {\em Probability Surveys}, 1:20--71, 2004.

\bibitem{schmidhuber2015deep}
J{\"u}rgen Schmidhuber.
\newblock Deep learning in neural networks: An overview.
\newblock {\em Neural Networks}, 61:85--117, 2015.

\bibitem{scott2016bayes}
Steven~L Scott, Alexander~W Blocker, Fernando~V Bonassi, Hugh~A Chipman,
  Edward~I George, and Robert~E McCulloch.
\newblock Bayes and big data: The consensus monte carlo algorithm.
\newblock {\em International Journal of Management Science and Engineering
  Management}, 11(2):78--88, 2016.

\bibitem{csimcsekli2016stochastic}
Umut {\c{S}}im{\c{s}}ekli, Roland Badeau, A~Taylan Cemgil, and Ga{\"e}l
  Richard.
\newblock Stochastic quasi-newton langevin monte carlo.
\newblock {\em arXiv preprint arXiv:1602.03442}, 2016.

\bibitem{teh2015distributed}
Yee~Whye Teh, Leonard Hasenclever, Thibaut Lienart, Sebastian Vollmer, Stefan
  Webb, Balaji Lakshminarayanan, and Charles Blundell.
\newblock Distributed bayesian learning with stochastic natural-gradient
  expectation propagation and the posterior server.
\newblock {\em arXiv preprint arXiv:1512.09327}, 2015.

\bibitem{teh2014consistency}
Yee~Whye Teh, Alexandre~H Thiery, and Sebastian~J Vollmer.
\newblock Consistency and fluctuations for stochastic gradient langevin
  dynamics.

\bibitem{tierney1994markov}
Luke Tierney.
\newblock Markov chains for exploring posterior distributions.
\newblock {\em the Annals of Statistics}, pages 1701--1728, 1994.

\bibitem{titterington2004bayesian}
DM~Titterington.
\newblock Bayesian methods for neural networks and related models.
\newblock {\em Statistical science}, pages 128--139, 2004.

\bibitem{vivarelli2001comparing}
Francesco Vivarelli and Christopher~KI Williams.
\newblock Comparing bayesian neural network algorithms for classifying
  segmented outdoor images.
\newblock {\em Neural Networks}, 14(4):427--437, 2001.

\bibitem{wang1990cluster}
Jian-Sheng Wang and Robert~H Swendsen.
\newblock Cluster monte carlo algorithms.
\newblock {\em Physica A: Statistical Mechanics and its Applications},
  167(3):565--579, 1990.

\bibitem{wang2013adaptive}
Ziyu Wang, Shakir Mohamed, and Nando De~Freitas.
\newblock Adaptive hamiltonian and riemann manifold monte carlo samplers.

\bibitem{watanabe2009algebraic}
Sumio Watanabe.
\newblock {\em Algebraic geometry and statistical learning theory}, volume~25.
\newblock Cambridge University Press, 2009.

\bibitem{watanabe2010asymptotic}
Sumio Watanabe.
\newblock Asymptotic equivalence of bayes cross validation and widely
  applicable information criterion in singular learning theory.
\newblock {\em Journal of Machine Learning Research}, 11(Dec):3571--3594, 2010.

\bibitem{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In {\em Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pages 681--688, 2011.

\bibitem{wright1999numerical}
Stephen Wright.
\newblock Numerical optimization.

\bibitem{yamada2012information}
Koshi Yamada and Sumio Watanabe.
\newblock Information criterion for variational bayes learning in regular and
  singular cases.
\newblock In {\em Soft Computing and Intelligent Systems (SCIS) and 13th
  International Symposium on Advanced Intelligent Systems (ISIS), 2012 Joint
  6th International Conference on}, pages 1551--1555. IEEE, 2012.

\bibitem{zhang2014semi}
Yichuan Zhang and Charles Sutton.
\newblock Semi-separable hamiltonian monte carlo for inference in bayesian
  hierarchical models.
\newblock In {\em Advances in neural information processing systems}, pages
  10--18, 2014.

\end{thebibliography}
